{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a685e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the project\n",
    "\n",
    "# System libraries\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np  # Efficient array operations\n",
    "import pandas as pd  # Data structures for working with structured data\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt  # Plotting and data visualization\n",
    "import seaborn as sns  # Further data visualization\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scales data to a range [0, 1]\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf  # Core machine learning framework\n",
    "from keras.api.models import Model  # Model class\n",
    "from keras.api.layers import Input, Dense  # Layers for the model\n",
    "from keras.api.optimizers import Adam  # Optimizer for the model\n",
    "from keras.api.losses import MeanSquaredError  # Loss function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0141459-855c-4fb6-b2c3-ffc6e6639c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for the project\n",
    "\n",
    "# Function to analyze a dataset (statistical analysis)\n",
    "def analyze_dataset(train_data, test_data, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis of the dataset, including:\n",
    "    - Shape and data types\n",
    "    - Missing values\n",
    "    - Summary statistics\n",
    "\n",
    "    Parameters:\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\nðŸŽ¯ Dataset Analysis ðŸŽ¯\\n\")\n",
    "    # Convert to DataFrame for better analysis\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    train_labels_df = pd.DataFrame(train_labels, columns=[''])\n",
    "    test_labels_df = pd.DataFrame(test_labels, columns=[''])\n",
    "\n",
    "    # Dataset Shape and Data Types\n",
    "    print(\"\\nðŸ”¹ Dataset Shape & Data Types:\\n\")\n",
    "    print(f\"Train data shape: {train_data.shape}, Type: {train_data.dtype}\")\n",
    "    print(f\"Test data shape: {test_data.shape}, Type: {test_data.dtype}\")\n",
    "    print(f\"Train labels shape: {train_labels.shape}, Type: {train_labels.dtype}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}, Type: {test_labels.dtype}\")\n",
    "\n",
    "    # Checking for Missing Values\n",
    "    print(\"\\nðŸ”¹ Missing Values:\\n\")\n",
    "    print(f\"Train data missing values: {np.isnan(train_data).sum()}\")\n",
    "    print(f\"Test data missing values: {np.isnan(test_data).sum()}\")\n",
    "    print(f\"Train labels missing values: {np.isnan(train_labels).sum()}\")\n",
    "    print(f\"Test labels missing values: {np.isnan(test_labels).sum()}\")\n",
    "\n",
    "    # Summary Statistics (using DataFrame)\n",
    "    print(\"\\nðŸ”¹ Statistical Summary:\\n\")\n",
    "    print(\"\\nTrain Data Statistics:\\n\\n\", train_df.describe())\n",
    "    print(\"\\nTest Data Statistics:\\n\\n\", test_df.describe())\n",
    "    print(\"\\nTrain Labels Statistics:\\n\", train_labels_df.describe())\n",
    "    print(\"\\nTest Labels Statistics:\\n\", test_labels_df.describe())\n",
    "\n",
    "# Function to visualize a dataset (plotting)\n",
    "def visualize_dataset(train_data, test_data, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Visualize the dataset by plotting:\n",
    "    - Feature distributions\n",
    "    - Correlation heatmap\n",
    "    - Outlier detection (boxplots)\n",
    "    - Label distribution\n",
    "\n",
    "    Parameters:\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\nðŸŽ¯ Dataset Visualization ðŸŽ¯\\n\")\n",
    "\n",
    "    # Feature Distributions\n",
    "    num_features = train_data.shape[1]\n",
    "    plt.figure(figsize=(15, num_features * 2))\n",
    "    for i in range(num_features):\n",
    "        plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "        sns.histplot(train_data[:, i], kde=True, bins=30, color=\"blue\", label=\"Train\")\n",
    "        sns.histplot(test_data[:, i], kde=True, bins=30, color=\"orange\", label=\"Test\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "    plt.suptitle(\"Feature Distributions (Train vs. Test)\\n\\n\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr_matrix = pd.DataFrame(train_data).corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Feature Correlation Heatmap\\n\")\n",
    "    plt.show()\n",
    "\n",
    "    # Outlier Detection (Boxplots)\n",
    "    plt.figure(figsize=(15, num_features * 2))\n",
    "    for i in range(num_features):\n",
    "        plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "        sns.boxplot(x=train_data[:, i], color=\"red\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "    plt.suptitle(\"Feature Outlier Detection (Boxplots)\\n\\n\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Label Distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(train_labels, kde=True, bins=30, color=\"blue\", label=\"Train Labels\")\n",
    "    sns.histplot(test_labels, kde=True, bins=30, color=\"orange\", label=\"Test Labels\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Label Distribution (Train vs. Test)\\n\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate a regression model (actual vs. predicted)\n",
    "def evaluate_model(model, train_data, test_data, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Visualize actual vs. predicted values for both training and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained regression model (callable or with `predict()` method)\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\nðŸŽ¯ Model Evaluation ðŸŽ¯\\n\")\n",
    "\n",
    "    # Predict values\n",
    "    train_preds = model.predict(train_data)\n",
    "    test_preds = model.predict(test_data)\n",
    "\n",
    "    # Number of samples to visualize\n",
    "    num_samples = min(30, len(train_labels), len(test_labels))\n",
    "\n",
    "    # Plot setup\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot train data\n",
    "    axes[0].plot(train_labels[:num_samples], \"r-\", label=\"True\", alpha=0.7)\n",
    "    axes[0].plot(train_preds[:num_samples], \"b-\", label=\"Predicted\", alpha=0.7)\n",
    "    axes[0].set_title(\"Train Data: Actual vs. Predicted\")\n",
    "    axes[0].set_xlabel(\"Sample Index\")\n",
    "    axes[0].set_ylabel(\"Value\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Plot test data\n",
    "    axes[1].plot(test_labels[:num_samples], \"r-\", label=\"True\", alpha=0.7)\n",
    "    axes[1].plot(test_preds[:num_samples], \"b-\", label=\"Predicted\", alpha=0.7)\n",
    "    axes[1].set_title(\"Test Data: Actual vs. Predicted\")\n",
    "    axes[1].set_xlabel(\"Sample Index\")\n",
    "    axes[1].set_ylabel(\"Value\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Display plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize model training history\n",
    "def visualize_model_history(model_history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation metrics of a Keras model.\n",
    "\n",
    "    Parameters:\n",
    "    model_history (History): The History object returned by the fit method of a Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\nðŸŽ¯ Training History Visualization ðŸŽ¯\\n\")\n",
    "\n",
    "    # Convert the history.history dictionary to a DataFrame\n",
    "    history_df = pd.DataFrame(model_history.history)\n",
    "\n",
    "    # Rename columns for better readability\n",
    "    history_df.rename(columns={\n",
    "        'loss': 'Training Loss',\n",
    "        'val_loss': 'Validation Loss'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Plot the DataFrame\n",
    "    history_df.plot(figsize=(10, 6))\n",
    "    plt.title('Model Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Function measure the accuracy of a regression model\n",
    "def analyze_model_accuracy(model, test_data, test_labels, error_threshold):\n",
    "    \"\"\"\n",
    "    Analyze the accuracy of a regression model by comparing predictions with actual values.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained regression model (callable or with `predict()` method)\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "        error_threshold (float): Threshold for considering a prediction as an error\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): The accuracy of the model\n",
    "        num_errors (int): The number of errors above the threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\nðŸŽ¯ Model Accuracy Analysis ðŸŽ¯\\n\")\n",
    "\n",
    "    # Predict values\n",
    "    model_predictions = model.predict(test_data)\n",
    "\n",
    "    # Initialize error counter\n",
    "    num_errors = 0\n",
    "\n",
    "    # Iterate over predictions and compare with actual values\n",
    "    print(f\"\\nðŸ”¹ Model errors above {error_threshold} (threshold):\\n\")\n",
    "    for index in range(len(model_predictions)):\n",
    "        if abs(model_predictions[index] - test_labels[index]) > error_threshold:\n",
    "            print(f\"Prediction: {model_predictions[index]}, Actual: {test_labels[index]}\")\n",
    "            num_errors += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 1.0 - (num_errors / len(model_predictions))\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nðŸ”¹ Model Accuracy Summary:\\n\")\n",
    "    print(f\"Number of errors: {num_errors}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Return accuracy and number of errors\n",
    "    return(accuracy)\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Function to log experiment results with automatic extraction\n",
    "def add_experiment_result(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    test_data,\n",
    "    test_labels,\n",
    "    model,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    model_history,\n",
    "    accuracy\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts experiment parameters and results from the model and history,\n",
    "    then logs them into a CSV file.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”¹ Experiment Results Logging\\n\")\n",
    "\n",
    "    # Generate a unique identifier using current date and time\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Extract training parameters\n",
    "    learning_rate = getattr(model.optimizer, \"learning_rate\", None)\n",
    "    if hasattr(learning_rate, \"numpy\"):\n",
    "        learning_rate = learning_rate.numpy()  # Convert Tensor to float\n",
    "\n",
    "    optimizer = type(model.optimizer).__name__\n",
    "\n",
    "    # Extract model architecture details\n",
    "    dense_layers = [layer for layer in model.layers if layer.__class__.__name__ == \"Dense\"]\n",
    "    if dense_layers:\n",
    "        activation_function = dense_layers[0].activation.__name__\n",
    "        num_units = dense_layers[0].units\n",
    "        num_layers = len(dense_layers)\n",
    "    else:\n",
    "        activation_function = None\n",
    "        num_units = None\n",
    "        num_layers = len(model.layers)\n",
    "\n",
    "    # Extract evaluation metrics\n",
    "    final_loss = model_history.history[\"loss\"][-1]\n",
    "    min_loss = min(model_history.history[\"loss\"])\n",
    "    max_loss = max(model_history.history[\"loss\"])\n",
    "    final_val_loss = model_history.history.get(\"val_loss\", [None])[-1]\n",
    "\n",
    "    # Prepare row data\n",
    "    row_data = {\n",
    "        \"Timestamp\": timestamp,\n",
    "        \"Batch Size\": batch_size,\n",
    "        \"Epochs\": epochs,\n",
    "        \"Learning Rate\": learning_rate,\n",
    "        \"Optimizer\": optimizer,\n",
    "        \"Activation Function\": activation_function,\n",
    "        \"Number of Layers\": num_layers,\n",
    "        \"Number of Units\": num_units,\n",
    "        \"Loss\": final_loss,\n",
    "        \"Minimum Loss\": min_loss,\n",
    "        \"Maximum Loss\": max_loss,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Validation Loss\": final_val_loss\n",
    "    }\n",
    "\n",
    "    # Print values being logged\n",
    "    for key, value in row_data.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Define CSV file path (globally configurable)\n",
    "    csv_path = os.path.expanduser(\"/home/saeed/projects/ml/src/mr-engineer-playverse/boston-housing/experiment_results.csv\")\n",
    "\n",
    "    # Load existing CSV or create new DataFrame\n",
    "    try:\n",
    "        experiment_results = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        experiment_results = pd.DataFrame(columns=row_data.keys())\n",
    "\n",
    "    # Ensure new row and experiment_results have matching columns\n",
    "    new_row = pd.DataFrame([row_data])\n",
    "    for col in new_row.columns:\n",
    "        if col not in experiment_results.columns:\n",
    "            experiment_results[col] = pd.NA\n",
    "\n",
    "    # âœ… Fix: Drop all-NA columns before concatenation to avoid warnings\n",
    "    experiment_results = pd.concat([experiment_results.dropna(axis=1, how='all'), new_row], ignore_index=True)\n",
    "\n",
    "    # Ensure directory exists and save the file\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    experiment_results.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3a561-9257-4255-a79c-1a313d637bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the Boston Housing dataset\n",
    "\n",
    "# Automatically splits into training and test sets (features and labels)\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# Analyze the dataset before preprocessing\n",
    "analyze_dataset(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9418d3-b387-4761-8855-aa2ff62d59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing steps for regression models\n",
    "\n",
    "#Print header for preprocessing steps\n",
    "print(\"\\nðŸŽ¯ Preprocessing Steps ðŸŽ¯\\n\")\n",
    "\n",
    "# Convert labels to a 2D array with shape (-1, 1) to ensure compatibility with models\n",
    "train_labels = np.reshape(train_labels, newshape=(-1, 1))\n",
    "test_labels = np.reshape(test_labels, newshape=(-1, 1))\n",
    "print(\"\\nðŸ”¹ Shapes After Reshaping:\")\n",
    "print(\"\\nTrain Labels Shape:\", train_labels.shape)\n",
    "print(\"Test Labels Shape:\", test_labels.shape)\n",
    "\n",
    "# Check min/max values for training and test data before normalization\n",
    "train_data_min, train_data_max = train_data.min(axis=0), train_data.max(axis=0)\n",
    "test_data_min, test_data_max = test_data.min(axis=0), test_data.max(axis=0)\n",
    "train_labels_min, train_labels_max = train_labels.min(axis=0), train_labels.max(axis=0)\n",
    "test_labels_min, test_labels_max = test_labels.min(axis=0), test_labels.max(axis=0)\n",
    "print(\"\\nðŸ”¹ Pre-Normalization Data Ranges:\")\n",
    "print(\"\\nTrain Data Min:\", train_data_min, \"\\nTrain Data Max:\", train_data_max)\n",
    "print(\"Test Data Min:\", test_data_min, \"\\nTest Data Max:\", test_data_max)\n",
    "\n",
    "# Fit the scaler on training data only\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(train_data)\n",
    "\n",
    "# Transform both training and test data using the scaler\n",
    "train_data = min_max_scaler.transform(train_data)\n",
    "test_data = min_max_scaler.transform(test_data)\n",
    "\n",
    "# Check min/max values for training and test data after normalization\n",
    "train_min_post, train_max_post = train_data.min(axis=0), train_data.max(axis=0)\n",
    "test_min_post, test_max_post = test_data.min(axis=0), test_data.max(axis=0)\n",
    "print(\"\\nðŸ”¹ Post-Normalization Data Ranges:\")\n",
    "print(\"\\nPost-Normalization Train Data Min:\", train_min_post, \"\\nPost-Normalization Train Data Max:\", train_max_post)\n",
    "print(\"Post-Normalization Test Data Min:\", test_min_post, \"\\nPost-Normalization Test Data Max:\", test_max_post)\n",
    "\n",
    "# Check min/max values for training and test labels\n",
    "print(\"\\nðŸ”¹ (Optional) Label Ranges:\")\n",
    "print(\"Train Labels Min:\", train_labels_min, \"\\nTrain Labels Max:\", train_labels_max)\n",
    "print(\"Test Labels Min:\", test_labels_min, \"\\nTest Labels Max:\", test_labels_max)\n",
    "\n",
    "# Convert dataset values to float32tra to optimize memory usage and computation speed\n",
    "train_data = train_data.astype(np.float32)\n",
    "test_data = test_data.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "print(\"\\nðŸ”¹ Data Types After Conversion:\")\n",
    "print(\"\\nTrain Data Type:\", train_data.dtype)\n",
    "print(\"Test Data Type:\", test_data.dtype)\n",
    "print(\"Train Labels Type:\", train_labels.dtype)\n",
    "print(\"Test Labels Type:\", test_labels.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ddf4f-51d8-4cd4-be99-10a6a9aa6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression model with minimal architecture\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\nðŸŽ¯ Regression Model Creation ðŸŽ¯\\n\")\n",
    "\n",
    "# Define input layer with 13 features\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Hidden layers with ReLU activation function\n",
    "first_layer = Dense(units=4, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer with linear activation\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36530b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression model with 32 batch size for input\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\nðŸŽ¯ Regression Model Creation ðŸŽ¯\\n\")\n",
    "\n",
    "# Define input layer with 13 features and batch size of 32\n",
    "input_layer = Input(shape=(13,), batch_size=32)\n",
    "\n",
    "# Hidden layers with ReLU activation function\n",
    "first_layer = Dense(units=4, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer with linear activation\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression model with 8 units in the hidden layer\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\nðŸŽ¯ Regression Model Creation ðŸŽ¯\\n\")\n",
    "\n",
    "# Define input layer with 13 features\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Hidden layers with ReLU activation function and 8 units\n",
    "first_layer = Dense(units=8, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer with linear activation\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the regression model with Adam optimizer and mean squared error loss\n",
    "print(\"\\nðŸŽ¯ Model Compilation ðŸŽ¯\\n\")\n",
    "model.compile(optimizer=Adam(), loss=MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the regression model on the training data\n",
    "\n",
    "# Set batch size and number of epochs for training\n",
    "model_batch_size = 8\n",
    "model_epochs = 100\n",
    "\n",
    "# Evaluate the model before training\n",
    "evaluate_model(model, train_data, test_data, train_labels, test_labels)\n",
    "\n",
    "# Train the model for 50 epochs with a batch size of 8\n",
    "print(\"\\nðŸŽ¯ Model Training ðŸŽ¯\\n\")\n",
    "model_history = model.fit(x=train_data, y=train_labels, epochs=model_epochs, batch_size=model_batch_size, validation_data=(test_data, test_labels))\n",
    "\n",
    "# Evaluate the model after training\n",
    "evaluate_model(model, train_data, test_data, train_labels, test_labels)\n",
    "\n",
    "# Visualize the model training history\n",
    "visualize_model_history(model_history)\n",
    "\n",
    "# Analyze the accuracy of the regression model\n",
    "model_accuracy = analyze_model_accuracy(model, test_data, test_labels, error_threshold=5.0)\n",
    "\n",
    "# Add the experiment result to the experiment results csv file\n",
    "add_experiment_result(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    test_data,\n",
    "    test_labels,\n",
    "    model,\n",
    "    model_batch_size,\n",
    "    model_epochs,\n",
    "    model_history,\n",
    "    model_accuracy,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
