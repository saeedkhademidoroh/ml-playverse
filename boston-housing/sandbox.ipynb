{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a685e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the project\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np  # Efficient array operations\n",
    "import pandas as pd  # Data structures for working with structured data\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt  # Plotting and data visualization\n",
    "import seaborn as sns  # Further data visualization\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scales data to a range [0, 1]\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf  # Core machine learning framework\n",
    "from keras.api.models import Model  # Model class\n",
    "from keras.api.layers import Input, Dense  # Layers for the model\n",
    "from keras.api.optimizers import Adam  # Optimizer for the model\n",
    "from keras.api.losses import MeanSquaredError  # Loss function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0141459-855c-4fb6-b2c3-ffc6e6639c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for the project\n",
    "\n",
    "# Function to analyze a dataset (statistical analysis)\n",
    "def analyze_dataset(train_data, test_data, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis of the dataset, including:\n",
    "    - Shape and data types\n",
    "    - Missing values\n",
    "    - Summary statistics\n",
    "\n",
    "    Parameters:\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\n<<Dataset Analysis>>\\n\")\n",
    "\n",
    "    # Convert to DataFrame for better analysis\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    train_labels_df = pd.DataFrame(train_labels, columns=[''])\n",
    "    test_labels_df = pd.DataFrame(test_labels, columns=[''])\n",
    "\n",
    "    # Dataset Shape and Data Types\n",
    "    print(\"\\n# Dataset Shape & Data Types:\\n\")\n",
    "    print(f\"Train data shape: {train_data.shape}, Type: {train_data.dtype}\")\n",
    "    print(f\"Test data shape: {test_data.shape}, Type: {test_data.dtype}\")\n",
    "    print(f\"Train labels shape: {train_labels.shape}, Type: {train_labels.dtype}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}, Type: {test_labels.dtype}\")\n",
    "\n",
    "    # Checking for Missing Values\n",
    "    print(\"\\n# Missing Values:\\n\")\n",
    "    print(f\"Train data missing values: {np.isnan(train_data).sum()}\")\n",
    "    print(f\"Test data missing values: {np.isnan(test_data).sum()}\")\n",
    "    print(f\"Train labels missing values: {np.isnan(train_labels).sum()}\")\n",
    "    print(f\"Test labels missing values: {np.isnan(test_labels).sum()}\")\n",
    "\n",
    "    # Summary Statistics (using DataFrame)\n",
    "    print(\"\\n# Statistical Summary:\\n\")\n",
    "    print(\"\\nTrain Data Statistics:\\n\\n\", train_df.describe())\n",
    "    print(\"\\nTest Data Statistics:\\n\\n\", test_df.describe())\n",
    "    print(\"\\nTrain Labels Statistics:\\n\", train_labels_df.describe())\n",
    "    print(\"\\nTest Labels Statistics:\\n\", test_labels_df.describe())\n",
    "\n",
    "# Function to visualize a dataset (plotting)\n",
    "def visualize_dataset(train_data, test_data, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Visualize the dataset by plotting:\n",
    "    - Feature distributions\n",
    "    - Correlation heatmap\n",
    "    - Outlier detection (boxplots)\n",
    "    - Label distribution\n",
    "\n",
    "    Parameters:\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\n<<Dataset Visualization>>\\n\")\n",
    "\n",
    "    # Feature Distributions\n",
    "    num_features = train_data.shape[1]\n",
    "    plt.figure(figsize=(15, num_features * 2))\n",
    "    for i in range(num_features):\n",
    "        plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "        sns.histplot(train_data[:, i], kde=True, bins=30, color=\"blue\", label=\"Train\")\n",
    "        sns.histplot(test_data[:, i], kde=True, bins=30, color=\"orange\", label=\"Test\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "    plt.suptitle(\"Feature Distributions (Train vs. Test)\\n\\n\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr_matrix = pd.DataFrame(train_data).corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Feature Correlation Heatmap\\n\")\n",
    "    plt.show()\n",
    "\n",
    "    # Outlier Detection (Boxplots)\n",
    "    plt.figure(figsize=(15, num_features * 2))\n",
    "    for i in range(num_features):\n",
    "        plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "        sns.boxplot(x=train_data[:, i], color=\"red\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "    plt.suptitle(\"Feature Outlier Detection (Boxplots)\\n\\n\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Label Distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(train_labels, kde=True, bins=30, color=\"blue\", label=\"Train Labels\")\n",
    "    sns.histplot(test_labels, kde=True, bins=30, color=\"orange\", label=\"Test Labels\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Label Distribution (Train vs. Test)\\n\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate a regression model (actual vs. predicted)\n",
    "def evaluate_model(model, train_data, test_data, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Visualize actual vs. predicted values for both training and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained regression model (callable or with `predict()` method)\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\n<<Model Evaluation>>\\n\")\n",
    "\n",
    "    # Predict values\n",
    "    train_preds = model.predict(train_data)\n",
    "    test_preds = model.predict(test_data)\n",
    "\n",
    "    # Number of samples to visualize\n",
    "    num_samples = min(30, len(train_labels), len(test_labels))\n",
    "\n",
    "    # Plot setup\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot train data\n",
    "    axes[0].plot(train_labels[:num_samples], \"r-\", label=\"True\", alpha=0.7)\n",
    "    axes[0].plot(train_preds[:num_samples], \"b-\", label=\"Predicted\", alpha=0.7)\n",
    "    axes[0].set_title(\"Train Data: Actual vs. Predicted\")\n",
    "    axes[0].set_xlabel(\"Sample Index\")\n",
    "    axes[0].set_ylabel(\"Value\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Plot test data\n",
    "    axes[1].plot(test_labels[:num_samples], \"r-\", label=\"True\", alpha=0.7)\n",
    "    axes[1].plot(test_preds[:num_samples], \"b-\", label=\"Predicted\", alpha=0.7)\n",
    "    axes[1].set_title(\"Test Data: Actual vs. Predicted\")\n",
    "    axes[1].set_xlabel(\"Sample Index\")\n",
    "    axes[1].set_ylabel(\"Value\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Display plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize model training history\n",
    "def visualize_model_history(model_history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation metrics of a Keras model.\n",
    "\n",
    "    Parameters:\n",
    "    model_history (History): The History object returned by the fit method of a Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\n<<Training History Visualization>>\\n\")\n",
    "\n",
    "    # Convert the history.history dictionary to a DataFrame\n",
    "    history_df = pd.DataFrame(model_history.history)\n",
    "\n",
    "    # Rename columns for better readability\n",
    "    history_df.rename(columns={\n",
    "        'loss': 'Training Loss',\n",
    "        'val_loss': 'Validation Loss'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Plot the DataFrame\n",
    "    history_df.plot(figsize=(10, 6))\n",
    "    plt.title('Model Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Function measure the accuracy of a regression model\n",
    "def analyze_model_accuracy(model, test_data, test_labels, error_threshold):\n",
    "    \"\"\"\n",
    "    Analyze the accuracy of a regression model by comparing predictions with actual values.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained regression model (callable or with `predict()` method)\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "        error_threshold (float): Threshold for considering a prediction as an error\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): The accuracy of the model\n",
    "        num_errors (int): The number of errors above the threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # Print header for the function\n",
    "    print(\"\\n<<Model Accuracy Analysis>>\\n\")\n",
    "\n",
    "    # Predict values\n",
    "    model_predictions = model.predict(test_data)\n",
    "\n",
    "    # Initialize error counter\n",
    "    num_errors = 0\n",
    "\n",
    "    # Iterate over predictions and compare with actual values\n",
    "    print(f\"\\n# Model errors above {error_threshold} (threshold):\\n\")\n",
    "    for index in range(len(model_predictions)):\n",
    "        if abs(model_predictions[index] - test_labels[index]) > error_threshold:\n",
    "            print(f\"Prediction: {model_predictions[index]}, Actual: {test_labels[index]}\")\n",
    "            num_errors += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 1.0 - (num_errors / len(model_predictions))\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n# Model Accuracy Summary:\\n\")\n",
    "    print(f\"Number of errors: {num_errors}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3a561-9257-4255-a79c-1a313d637bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the Boston Housing dataset\n",
    "\n",
    "# Automatically splits into training and test sets (features and labels)\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# Analyze the dataset before preprocessing\n",
    "analyze_dataset(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9418d3-b387-4761-8855-aa2ff62d59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing steps for regression models\n",
    "\n",
    "#Print header for preprocessing steps\n",
    "print(\"\\n<<Preprocessing Steps>>\\n\")\n",
    "\n",
    "# Convert labels to a 2D array with shape (-1, 1) to ensure compatibility with models\n",
    "train_labels = np.reshape(train_labels, newshape=(-1, 1))\n",
    "test_labels = np.reshape(test_labels, newshape=(-1, 1))\n",
    "print(\"\\n# Shapes After Reshaping:\")\n",
    "print(\"\\nTrain Labels Shape:\", train_labels.shape)\n",
    "print(\"Test Labels Shape:\", test_labels.shape)\n",
    "\n",
    "# Check min/max values for training and test data before normalization\n",
    "train_data_min, train_data_max = train_data.min(axis=0), train_data.max(axis=0)\n",
    "test_data_min, test_data_max = test_data.min(axis=0), test_data.max(axis=0)\n",
    "train_labels_min, train_labels_max = train_labels.min(axis=0), train_labels.max(axis=0)\n",
    "test_labels_min, test_labels_max = test_labels.min(axis=0), test_labels.max(axis=0)\n",
    "print(\"\\n# Pre-Normalization Data Ranges:\")\n",
    "print(\"\\nTrain Data Min:\", train_data_min, \"\\nTrain Data Max:\", train_data_max)\n",
    "print(\"Test Data Min:\", test_data_min, \"\\nTest Data Max:\", test_data_max)\n",
    "\n",
    "# Fit the scaler on training data only\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(train_data)\n",
    "\n",
    "# Transform both training and test data using the scaler\n",
    "train_data = min_max_scaler.transform(train_data)\n",
    "test_data = min_max_scaler.transform(test_data)\n",
    "\n",
    "# Check min/max values for training and test data after normalization\n",
    "train_min_post, train_max_post = train_data.min(axis=0), train_data.max(axis=0)\n",
    "test_min_post, test_max_post = test_data.min(axis=0), test_data.max(axis=0)\n",
    "print(\"\\n# Post-Normalization Data Ranges:\")\n",
    "print(\"\\nPost-Normalization Train Data Min:\", train_min_post, \"\\nPost-Normalization Train Data Max:\", train_max_post)\n",
    "print(\"Post-Normalization Test Data Min:\", test_min_post, \"\\nPost-Normalization Test Data Max:\", test_max_post)\n",
    "\n",
    "# Check min/max values for training and test labels\n",
    "print(\"\\n# (Optional) Label Ranges:\")\n",
    "print(\"Train Labels Min:\", train_labels_min, \"\\nTrain Labels Max:\", train_labels_max)\n",
    "print(\"Test Labels Min:\", test_labels_min, \"\\nTest Labels Max:\", test_labels_max)\n",
    "\n",
    "# Convert dataset values to float32tra to optimize memory usage and computation speed\n",
    "train_data = train_data.astype(np.float32)\n",
    "test_data = test_data.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "print(\"\\n# Data Types After Conversion:\")\n",
    "print(\"\\nTrain Data Type:\", train_data.dtype)\n",
    "print(\"Test Data Type:\", test_data.dtype)\n",
    "print(\"Train Labels Type:\", train_labels.dtype)\n",
    "print(\"Test Labels Type:\", test_labels.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ddf4f-51d8-4cd4-be99-10a6a9aa6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression model with minimal architecture\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\n<<Regression Model Creation>>\\n\")\n",
    "\n",
    "# Define input layer with 13 features\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Hidden layers with ReLU activation function\n",
    "first_layer = Dense(units=4, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer with linear activation\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "regression_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36530b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression model with 32 batch size for input\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\n<<Regression Model Creation>>\\n\")\n",
    "\n",
    "# Define input layer with 13 features and batch size of 32\n",
    "input_layer = Input(shape=(13,), batch_size=32)\n",
    "\n",
    "# Hidden layers with ReLU activation function\n",
    "first_layer = Dense(units=4, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer with linear activation\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "regression_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression model with 8 units in the hidden layer\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\n<<Regression Model Creation>>\\n\")\n",
    "\n",
    "# Define input layer with 13 features\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Hidden layers with ReLU activation function and 8 units\n",
    "first_layer = Dense(units=8, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer with linear activation\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "regression_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the regression model with Adam optimizer and mean squared error loss\n",
    "print(\"\\n<<Model Compilation>>\\n\")\n",
    "regression_model.compile(optimizer=Adam(), loss=MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the regression model on the training data\n",
    "\n",
    "# Evaluate the model before training\n",
    "evaluate_model(regression_model, train_data, test_data, train_labels, test_labels)\n",
    "\n",
    "# Train the model for 50 epochs with a batch size of 8\n",
    "print(\"\\n<<Model Training>>\\n\")\n",
    "model_history = regression_model.fit(x=train_data, y=train_labels, epochs=50, batch_size=8, validation_data=(test_data, test_labels))\n",
    "\n",
    "# Evaluate the model after training\n",
    "evaluate_model(regression_model, train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9495c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model training history\n",
    "visualize_model_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acdb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the accuracy of the regression model\n",
    "analyze_model_accuracy(regression_model, test_data, test_labels, error_threshold=5.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
