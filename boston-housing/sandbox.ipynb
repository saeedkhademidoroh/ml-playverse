{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a685e45",
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for project\n",
    "\n",
    "# System libraries\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np # Efficient array operations\n",
    "import pandas as pd # Data structures for working with structured data\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt # Plotting and data visualization\n",
    "import seaborn as sns # Further data visualization\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler # Scales data to a range [0, 1]\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf # Core machine learning framework\n",
    "from keras.api.models import Model # Model class\n",
    "from keras.api.layers import Input, Dense # Layers for model\n",
    "from keras.api.optimizers import Adam, SGD # Optimizer for model\n",
    "from keras.api.losses import MeanSquaredError # Loss function for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0141459-855c-4fb6-b2c3-ffc6e6639c89",
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "# Function definition for project\n",
    "\n",
    "# Function to analyze a dataset (statistical analysis)\n",
    "def analyze_dataset(train_data, train_labels, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis of dataset, including:\n",
    "    - Shape and data types\n",
    "    - Missing values\n",
    "    - Summary statistics\n",
    "\n",
    "    Parameters:\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Print header for function\n",
    "    print(\"\\nðŸŽ¯ Dataset Analysis ðŸŽ¯\\n\")\n",
    "\n",
    "    # Convert to DataFrame for better analysis\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    train_labels_df = pd.DataFrame(train_labels, columns=[''])\n",
    "    test_labels_df = pd.DataFrame(test_labels, columns=[''])\n",
    "\n",
    "    # Dataset Shape and Data Types\n",
    "    print(\"\\nðŸ”¹ Dataset Shape & Data Types:\\n\")\n",
    "    print(f\"Train data shape: {train_data.shape}, Type: {train_data.dtype}\")\n",
    "    print(f\"Test data shape: {test_data.shape}, Type: {test_data.dtype}\")\n",
    "    print(f\"Train labels shape: {train_labels.shape}, Type: {train_labels.dtype}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}, Type: {test_labels.dtype}\")\n",
    "\n",
    "    # Checking for Missing Values\n",
    "    print(\"\\nðŸ”¹ Missing Values:\\n\")\n",
    "    print(f\"Train data missing values: {np.isnan(train_data).sum()}\")\n",
    "    print(f\"Test data missing values: {np.isnan(test_data).sum()}\")\n",
    "    print(f\"Train labels missing values: {np.isnan(train_labels).sum()}\")\n",
    "    print(f\"Test labels missing values: {np.isnan(test_labels).sum()}\")\n",
    "\n",
    "    # Summary Statistics (using DataFrame)\n",
    "    print(\"\\nðŸ”¹ Statistical Summary:\\n\")\n",
    "    print(\"\\nTrain Data Statistics:\\n\\n\", train_df.describe())\n",
    "    print(\"\\nTest Data Statistics:\\n\\n\", test_df.describe())\n",
    "    print(\"\\nTrain Labels Statistics:\\n\", train_labels_df.describe())\n",
    "    print(\"\\nTest Labels Statistics:\\n\", test_labels_df.describe())\n",
    "\n",
    "\n",
    "# Function to preprocess a dataset (normalization, reshaping, etc.)\n",
    "def preprocess_dataset(train_data, train_labels, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Preprocesses data for models:\n",
    "    - Reshapes labels\n",
    "    - Prints pre-normalization min/max ranges\n",
    "    - Applies MinMaxScaler normalization\n",
    "    - Prints post-normalization min/max ranges\n",
    "    - Converts data types to float32 for optimization\n",
    "\n",
    "    Returns:\n",
    "    - Scaled train_data, train_labels, test_data, test_labels\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Print header for function\n",
    "    print(\"\\nðŸŽ¯ Preprocessing Steps ðŸŽ¯\\n\")\n",
    "\n",
    "    # Reshape labels to ensure compatibility\n",
    "    train_labels = np.reshape(train_labels, (-1, 1))\n",
    "    test_labels = np.reshape(test_labels, (-1, 1))\n",
    "\n",
    "    print(\"\\nðŸ”¹ Shapes After Reshaping:\")\n",
    "    print(\"Train Labels Shape:\", train_labels.shape)\n",
    "    print(\"Test Labels Shape:\", test_labels.shape)\n",
    "\n",
    "    # Check pre-normalization min/max values\n",
    "    train_data_min, train_data_max = train_data.min(axis=0), train_data.max(axis=0)\n",
    "    test_data_min, test_data_max = test_data.min(axis=0), test_data.max(axis=0)\n",
    "    train_labels_min, train_labels_max = train_labels.min(axis=0), train_labels.max(axis=0)\n",
    "    test_labels_min, test_labels_max = test_labels.min(axis=0), test_labels.max(axis=0)\n",
    "\n",
    "    print(\"\\nðŸ”¹ Pre-Normalization Data Ranges:\")\n",
    "    print(\"Train Data Min:\", train_data_min, \"\\nTrain Data Max:\", train_data_max)\n",
    "    print(\"Test Data Min:\", test_data_min, \"\\nTest Data Max:\", test_data_max)\n",
    "\n",
    "    # Fit scaler on training data only\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max_scaler.fit(train_data)\n",
    "\n",
    "    # Transform both training and test data using scaler\n",
    "    train_data = min_max_scaler.transform(train_data)\n",
    "    test_data = min_max_scaler.transform(test_data)\n",
    "\n",
    "    # Check post-normalization min/max values\n",
    "    train_min_post, train_max_post = train_data.min(axis=0), train_data.max(axis=0)\n",
    "    test_min_post, test_max_post = test_data.min(axis=0), test_data.max(axis=0)\n",
    "\n",
    "    print(\"\\nðŸ”¹ Post-Normalization Data Ranges:\")\n",
    "    print(\"Post-Normalization Train Data Min:\", train_min_post, \"\\nPost-Normalization Train Data Max:\", train_max_post)\n",
    "    print(\"Post-Normalization Test Data Min:\", test_min_post, \"\\nPost-Normalization Test Data Max:\", test_max_post)\n",
    "\n",
    "    # Print min/max values for labels\n",
    "    print(\"\\nðŸ”¹ (Optional) Label Ranges:\")\n",
    "    print(\"Train Labels Min:\", train_labels_min, \"\\nTrain Labels Max:\", train_labels_max)\n",
    "    print(\"Test Labels Min:\", test_labels_min, \"\\nTest Labels Max:\", test_labels_max)\n",
    "\n",
    "    # Convert dataset values to float32 for optimization\n",
    "    train_data = train_data.astype(np.float32)\n",
    "    test_data = test_data.astype(np.float32)\n",
    "    train_labels = train_labels.astype(np.float32)\n",
    "    test_labels = test_labels.astype(np.float32)\n",
    "\n",
    "    print(\"\\nðŸ”¹ Data Types After Conversion:\")\n",
    "    print(\"Train Data Type:\", train_data.dtype)\n",
    "    print(\"Test Data Type:\", test_data.dtype)\n",
    "    print(\"Train Labels Type:\", train_labels.dtype)\n",
    "    print(\"Test Labels Type:\", test_labels.dtype)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# Function to visualize a dataset (plotting)\n",
    "def visualize_dataset(train_data, train_labels, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Visualize dataset by plotting:\n",
    "    - Feature distributions\n",
    "    - Correlation heatmap\n",
    "    - Outlier detection (boxplots)\n",
    "    - Label distribution\n",
    "\n",
    "    Parameters:\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Print header for function\n",
    "    print(\"\\nðŸŽ¯ Dataset Visualization ðŸŽ¯\\n\")\n",
    "\n",
    "    # Feature Distributions\n",
    "    num_features = train_data.shape[1]\n",
    "    plt.figure(figsize=(15, num_features * 2))\n",
    "    for i in range(num_features):\n",
    "        plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "        sns.histplot(train_data[:, i], kde=True, bins=30, color=\"blue\", label=\"Train\")\n",
    "        sns.histplot(test_data[:, i], kde=True, bins=30, color=\"orange\", label=\"Test\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "    plt.suptitle(\"Feature Distributions (Train vs. Test)\\n\\n\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr_matrix = pd.DataFrame(train_data).corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Feature Correlation Heatmap\\n\")\n",
    "    plt.show()\n",
    "\n",
    "    # Outlier Detection (Boxplots)\n",
    "    plt.figure(figsize=(15, num_features * 2))\n",
    "    for i in range(num_features):\n",
    "        plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "        sns.boxplot(x=train_data[:, i], color=\"red\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "    plt.suptitle(\"Feature Outlier Detection (Boxplots)\\n\\n\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Label Distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(train_labels, kde=True, bins=30, color=\"blue\", label=\"Train Labels\")\n",
    "    sns.histplot(test_labels, kde=True, bins=30, color=\"orange\", label=\"Test Labels\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Label Distribution (Train vs. Test)\\n\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to evaluate a model (actual vs. predicted)\n",
    "def evaluate_model(model, train_data, train_labels, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Visualize actual vs. predicted values for both training and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model (callable or with `predict()` method)\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Print header for function\n",
    "    print(\"\\nðŸŽ¯ Model Evaluation ðŸŽ¯\\n\")\n",
    "\n",
    "    # Predict values\n",
    "    train_preds = model.predict(train_data)\n",
    "    test_preds = model.predict(test_data)\n",
    "\n",
    "    # Number of samples to visualize\n",
    "    num_samples = min(30, len(train_labels), len(test_labels))\n",
    "\n",
    "    # Plot setup\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot train data\n",
    "    axes[0].plot(train_labels[:num_samples], \"r-\", label=\"True\", alpha=0.7)\n",
    "    axes[0].plot(train_preds[:num_samples], \"b-\", label=\"Predicted\", alpha=0.7)\n",
    "    axes[0].set_title(\"Train Data: Actual vs. Predicted\")\n",
    "    axes[0].set_xlabel(\"Sample Index\")\n",
    "    axes[0].set_ylabel(\"Value\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Plot test data\n",
    "    axes[1].plot(test_labels[:num_samples], \"r-\", label=\"True\", alpha=0.7)\n",
    "    axes[1].plot(test_preds[:num_samples], \"b-\", label=\"Predicted\", alpha=0.7)\n",
    "    axes[1].set_title(\"Test Data: Actual vs. Predicted\")\n",
    "    axes[1].set_xlabel(\"Sample Index\")\n",
    "    axes[1].set_ylabel(\"Value\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Display plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to visualize model training history\n",
    "def visualize_model_history(model_history):\n",
    "    \"\"\"\n",
    "    Plots training and validation metrics of a Keras model.\n",
    "\n",
    "    Parameters:\n",
    "    model_history (History): History object returned by fit method of a Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Print header for function\n",
    "    print(\"\\nðŸŽ¯ Training History Visualization ðŸŽ¯\\n\")\n",
    "\n",
    "    # Convert history.history dictionary to a DataFrame\n",
    "    history_df = pd.DataFrame(model_history.history)\n",
    "\n",
    "    # Rename columns for better readability\n",
    "    history_df.rename(columns={\n",
    "        'loss': 'Training Loss',\n",
    "        'val_loss': 'Validation Loss'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Plot DataFrame\n",
    "    history_df.plot(figsize=(10, 6))\n",
    "    plt.title('Model Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to calculate accuracy of a model\n",
    "def calculate_model_accuracy(model, test_data, test_labels, threshold):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of a model by comparing predictions with actual values.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model (callable or with `predict()` method)\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "        error_threshold (float): Threshold for considering a prediction as an error\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): accuracy of model\n",
    "        num_errors (int): number of errors above threshold\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Print header for function\n",
    "    print(\"\\nðŸŽ¯ Model Accuracy Calculation ðŸŽ¯\\n\")\n",
    "\n",
    "    # Predict values\n",
    "    model_predictions = model.predict(test_data)\n",
    "\n",
    "    # Initialize error counter\n",
    "    num_errors = 0\n",
    "\n",
    "    # Iterate over predictions and compare with actual values\n",
    "    print(f\"\\nðŸ”¹ Model errors above {threshold} (threshold):\\n\")\n",
    "    for index in range(len(model_predictions)):\n",
    "        if abs(model_predictions[index] - (test_labels[index])) > threshold:\n",
    "            print(f\"Prediction: {model_predictions[index]}, Actual: {test_labels[index]}\")\n",
    "            num_errors += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 1.0 - (num_errors / len(model_predictions))\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nðŸ”¹ Model Accuracy Summary:\\n\")\n",
    "    print(f\"Number of errors: {num_errors}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Return accuracy and number of errors\n",
    "    return(accuracy)\n",
    "\n",
    "\n",
    "# Function to add experiment results to a csv file\n",
    "def add_experiment_result(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    test_data,\n",
    "    test_labels,\n",
    "    model,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    model_history,\n",
    "    threshold,\n",
    "    accuracy,\n",
    "    description=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts experiment parameters and results from model and history,\n",
    "    then logs them into a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Print header for function\n",
    "    print(\"\\nðŸŽ¯ Experiment Results Logging ðŸŽ¯\\n\")\n",
    "\n",
    "    # Extract model name\n",
    "    model_name = model.name\n",
    "\n",
    "    # Generate a unique identifier using current date and time\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Extract training parameters\n",
    "    learning_rate = getattr(model.optimizer, \"learning_rate\", None)\n",
    "    if hasattr(learning_rate, \"numpy\"):\n",
    "        learning_rate = learning_rate.numpy()  # Convert Tensor to float\n",
    "\n",
    "    # Extract optimizer details\n",
    "    optimizer = type(model.optimizer).__name__\n",
    "\n",
    "    # Extract model architecture details\n",
    "    dense_layers = [layer for layer in model.layers if layer.__class__.__name__ == \"Dense\"]\n",
    "    if dense_layers:\n",
    "        activation_function = dense_layers[0].activation.__name__\n",
    "        num_layers = len(dense_layers)\n",
    "        num_units = dense_layers[0].units\n",
    "    else:\n",
    "        activation_function = None\n",
    "        num_layers = len(model.layers)\n",
    "        num_units = None\n",
    "\n",
    "    # Extract evaluation metrics\n",
    "    final_loss = model_history.history[\"loss\"][-1]\n",
    "    min_loss = min(model_history.history[\"loss\"])\n",
    "    max_loss = max(model_history.history[\"loss\"])\n",
    "    final_val_loss = model_history.history.get(\"val_loss\", [None])[-1]\n",
    "\n",
    "    # Create a dictionary of extracted data\n",
    "    row_data = {\n",
    "        \"Name\": model_name,\n",
    "        \"Timestamp\": timestamp,\n",
    "        \"Batch Size\": batch_size,\n",
    "        \"Epochs\": epochs,\n",
    "        \"Learning Rate\": learning_rate,\n",
    "        \"Optimizer\": optimizer,\n",
    "        \"Activation Function\": activation_function,\n",
    "        \"Number of Layers\": num_layers,\n",
    "        \"Number of Units\": num_units,\n",
    "        \"Loss\": final_loss,\n",
    "        \"Minimum Loss\": min_loss,\n",
    "        \"Maximum Loss\": max_loss,\n",
    "        \"Validation Loss\": final_val_loss,\n",
    "        \"Error Threshold\": threshold,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Description\": description\n",
    "    }\n",
    "\n",
    "    # Print values being logged\n",
    "    print(\"\\nðŸ”¹ Experiment Results:\\n\")\n",
    "    for key, value in row_data.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Define CSV file path\n",
    "    csv_path = os.path.expanduser(\"/home/saeed/projects/ml/src/mr-engineer-playverse/boston-housing/experiment_results.csv\")\n",
    "\n",
    "    # Load existing CSV or create new DataFrame\n",
    "    try:\n",
    "        experiment_results = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        experiment_results = pd.DataFrame(columns=row_data.keys())\n",
    "\n",
    "    # Ensure new row and experiment_results have matching columns\n",
    "    new_row = pd.DataFrame([row_data])\n",
    "    for col in new_row.columns:\n",
    "        if col not in experiment_results.columns:\n",
    "            experiment_results[col] = pd.NA\n",
    "\n",
    "    # Append new row to DataFrame\n",
    "    experiment_results = pd.concat([new_row, experiment_results.dropna(axis=1, how=\"all\")], ignore_index=True)\n",
    "\n",
    "    # Save updated DataFrame to CSV and Excel\n",
    "    excel_path = csv_path.replace(\".csv\", \".xlsx\")\n",
    "    with pd.ExcelWriter(excel_path, engine=\"xlsxwriter\") as writer:\n",
    "        experiment_results.to_excel(writer, index=False, sheet_name=\"Results\")\n",
    "\n",
    "        # Get xlsxwriter workbook and worksheet objects\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[\"Results\"]\n",
    "\n",
    "        # Set column widths based on max length of data in each column\n",
    "        for col_idx, col in enumerate(experiment_results.columns):\n",
    "            max_length = max(experiment_results[col].astype(str).map(len).max(), len(col)) + 2\n",
    "            worksheet.set_column(col_idx, col_idx, max_length)\n",
    "\n",
    "        # Create a cell format for centering text horizontally and vertically\n",
    "        cell_format = workbook.add_format({'align': 'center', 'valign': 'vcenter'})\n",
    "\n",
    "        # Create a bold cell format for header\n",
    "        header_format = workbook.add_format({'align': 'center', 'valign': 'vcenter', 'bold': True})\n",
    "\n",
    "        # Write header with bold formatting\n",
    "        for col_idx in range(len(experiment_results.columns)):\n",
    "            worksheet.write(0, col_idx, experiment_results.columns[col_idx], header_format)\n",
    "\n",
    "        # Write data rows with formatting (starting from row 1)\n",
    "        for row_idx in range(len(experiment_results)):\n",
    "            for col_idx in range(len(experiment_results.columns)):\n",
    "                value = experiment_results.iloc[row_idx, col_idx]\n",
    "\n",
    "                # Convert NaN/Inf to a safe value\n",
    "                if pd.isna(value):  # Check for NaN\n",
    "                    value = \"N/A\"\n",
    "                elif value == np.inf:  # Check for positive infinity\n",
    "                    value = \"Infinity\"\n",
    "                elif value == -np.inf:  # Check for negative infinity\n",
    "                    value = \"-Infinity\"\n",
    "\n",
    "                worksheet.write(row_idx + 1, col_idx, value, cell_format)\n",
    "\n",
    "    # Ensure directory exists and save file\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    experiment_results.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9418d3-b387-4761-8855-aa2ff62d59bf",
   "metadata": {
    "tags": [
     "preprocess"
    ]
   },
   "outputs": [],
   "source": [
    "# Automatically splits into training and test sets (features and labels)\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# Analyze dataset before preprocessing\n",
    "analyze_dataset(train_data, train_labels, test_data, test_labels)\n",
    "\n",
    "# Preprocess dataset\n",
    "train_data, train_labels, test_data, test_labels = preprocess_dataset(train_data, train_labels, test_data, test_labels)\n",
    "\n",
    "# Analyze dataset after preprocessing\n",
    "analyze_dataset(train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dc7fc",
   "metadata": {
    "tags": [
     "train"
    ]
   },
   "outputs": [],
   "source": [
    "# Train model and store training history\n",
    "print(\"\\nðŸŽ¯ Model Training ðŸŽ¯\\n\")\n",
    "\n",
    "# Set training parameters\n",
    "batch_size = 8\n",
    "epochs = 200\n",
    "threshold = 5.0\n",
    "\n",
    "# Early stopping callback\n",
    "# early_stop = EarlyStopping(monitor=\"val_loss\", patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train model and store training history\n",
    "history = model.fit( # type: ignore\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_data, test_labels)\n",
    "    # callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Evaluate model after training\n",
    "evaluate_model(model, train_data, train_labels, test_data, test_labels) # type: ignore\n",
    "\n",
    "# Visualize model training history\n",
    "visualize_model_history(history)\n",
    "\n",
    "# Calculate accuracy of model\n",
    "accuracy = calculate_model_accuracy(model, test_data, test_labels, threshold=5.0) # type: ignore\n",
    "\n",
    "# Add experiment result to experiment results csv file\n",
    "add_experiment_result(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    test_data,\n",
    "    test_labels,\n",
    "    model, # type: ignore\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    history,\n",
    "    threshold,\n",
    "    accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ddf4f-51d8-4cd4-be99-10a6a9aa6f00",
   "metadata": {
    "tags": [
     "model_1"
    ]
   },
   "outputs": [],
   "source": [
    "# Create model 1\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\nðŸŽ¯ Regression Model Creation ðŸŽ¯\\n\")\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Define first layer\n",
    "first_layer = Dense(units=4, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Define output layer\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=input_layer, outputs=output_layer, name=\"m1\")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "print(\"\\nðŸŽ¯ Model Compilation ðŸŽ¯\\n\")\n",
    "model.compile(optimizer=Adam(), loss=MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828da1f",
   "metadata": {
    "tags": [
     "model_2"
    ]
   },
   "outputs": [],
   "source": [
    "# Create model 2\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\nðŸŽ¯ Regression Model Creation ðŸŽ¯\\n\")\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Define first layer\n",
    "first_layer = Dense(units=8, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Define output layer\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=input_layer, outputs=output_layer, name=\"m2\")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "print(\"\\nðŸŽ¯ Model Compilation ðŸŽ¯\\n\")\n",
    "model.compile(optimizer=Adam(), loss=MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeff002",
   "metadata": {
    "tags": [
     "model_3"
    ]
   },
   "outputs": [],
   "source": [
    "# Create model 3\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\nðŸŽ¯ Regression Model Creation ðŸŽ¯\\n\")\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Define first layer\n",
    "first_layer = Dense(units=8, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Define second layer\n",
    "second_layer = Dense(units=4, activation=\"relu\")(first_layer)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(units=1)(second_layer)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=input_layer, outputs=output_layer, name=\"m3\")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "print(\"\\nðŸŽ¯ Model Compilation ðŸŽ¯\\n\")\n",
    "model.compile(optimizer=Adam(), loss=MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9598253",
   "metadata": {
    "tags": [
     "model_4"
    ]
   },
   "outputs": [],
   "source": [
    "# Create model 4\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\nðŸŽ¯ Regression Model Creation ðŸŽ¯\\n\")\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Define first layer\n",
    "first_layer = Dense(units=8, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Define second layer\n",
    "second_layer = Dense(units=4, activation=\"relu\")(first_layer)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(units=1)(second_layer)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=input_layer, outputs=output_layer, name=\"m4\")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "print(\"\\nðŸŽ¯ Model Compilation ðŸŽ¯\\n\")\n",
    "model.compile(optimizer=SGD(0.01, momentum=0.9), loss=MeanSquaredError())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525b1d7",
   "metadata": {
    "tags": [
     "model_5"
    ]
   },
   "outputs": [],
   "source": [
    "# Create model 5\n",
    "\n",
    "# Print header for model creation\n",
    "print(\"\\nðŸŽ¯ Regression Model Creation ðŸŽ¯\\n\")\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Define first layer\n",
    "first_layer = Dense(units=8, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Define second layer\n",
    "second_layer = Dense(units=4, activation=\"relu\")(first_layer)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(units=1)(second_layer)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=input_layer, outputs=output_layer, name=\"m5\")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "print(\"\\nðŸŽ¯ Model Compilation ðŸŽ¯\\n\")\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4de727",
   "metadata": {
    "tags": [
     "model_6"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
