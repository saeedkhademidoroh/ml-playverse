{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a685e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "\n",
    "# 1.1. Standard Libraries\n",
    "\n",
    "# 1.2. Third-party Libraries\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np  # Efficient array operations\n",
    "import pandas as pd  # Data structures for working with structured data\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt  # Plotting and data visualization\n",
    "import seaborn as sns  # Further data visualization\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scales data to a range [0, 1]\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf  # Core machine learning framework\n",
    "\n",
    "# 1.3. Local Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0141459-855c-4fb6-b2c3-ffc6e6639c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Function Definitions\n",
    "\n",
    "# 2.1 Dataset Functions\n",
    "\n",
    "# Function to analyze a dataset\n",
    "def analyze_dataset(train_data, test_data, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Perform a complete analysis of the dataset, including:\n",
    "    - Shape and data types\n",
    "    - Missing values\n",
    "    - Statistical summaries\n",
    "    - Feature distributions\n",
    "    - Correlation heatmap\n",
    "    - Outlier detection\n",
    "    - Label distribution\n",
    "\n",
    "    Parameters:\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to DataFrame for better analysis\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    train_labels_df = pd.DataFrame(train_labels, columns=[''])\n",
    "    test_labels_df = pd.DataFrame(test_labels, columns=[''])\n",
    "\n",
    "    # Dataset Shape and Data Types\n",
    "    print(\"\\nðŸ”¹ Dataset Shape & Data Types:\")\n",
    "    print(f\"Train data shape: {train_data.shape}, Type: {train_data.dtype}\")\n",
    "    print(f\"Test data shape: {test_data.shape}, Type: {test_data.dtype}\")\n",
    "    print(f\"Train labels shape: {train_labels.shape}, Type: {train_labels.dtype}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}, Type: {test_labels.dtype}\")\n",
    "\n",
    "    # Checking for Missing Values\n",
    "    print(\"\\nðŸ”¹ Missing Values:\")\n",
    "    print(f\"Train data missing values: {np.isnan(train_data).sum()}\")\n",
    "    print(f\"Test data missing values: {np.isnan(test_data).sum()}\")\n",
    "    print(f\"Train labels missing values: {np.isnan(train_labels).sum()}\")\n",
    "    print(f\"Test labels missing values: {np.isnan(test_labels).sum()}\")\n",
    "\n",
    "    # Summary Statistics (using DataFrame)\n",
    "    print(\"\\nðŸ”¹ Summary Statistics:\")\n",
    "    print(\"\\nTrain Data Statistics:\\n\\n\", train_df.describe())\n",
    "    print(\"\\nTest Data Statistics:\\n\\n\", test_df.describe())\n",
    "    print(\"\\nTrain Labels Statistics:\\n\", train_labels_df.describe())\n",
    "    print(\"\\nTest Labels Statistics:\\n\", test_labels_df.describe())\n",
    "\n",
    "    # # Feature Distributions\n",
    "    # num_features = train_data.shape[1]\n",
    "    # plt.figure(figsize=(15, num_features * 2))\n",
    "    # for i in range(num_features):\n",
    "    #     plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "    #     sns.histplot(train_data[:, i], kde=True, bins=30, color=\"blue\", label=\"Train\")\n",
    "    #     sns.histplot(test_data[:, i], kde=True, bins=30, color=\"orange\", label=\"Test\")\n",
    "    #     plt.xlabel(f\"Feature {i}\")\n",
    "    #     plt.ylabel(\"Count\")\n",
    "    #     plt.legend()\n",
    "    # plt.suptitle(\"Feature Distributions (Train vs. Test)\\n\\n\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # # Correlation Heatmap\n",
    "    # plt.figure(figsize=(12, 8))\n",
    "    # corr_matrix = train_df.corr()\n",
    "    # sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    # plt.title(\"Feature Correlation Heatmap\\n\")\n",
    "    # plt.show()\n",
    "\n",
    "    # # Outlier Detection (Boxplots)\n",
    "    # plt.figure(figsize=(15, num_features * 2))\n",
    "    # for i in range(num_features):\n",
    "    #     plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "    #     sns.boxplot(x=train_data[:, i], color=\"red\")\n",
    "    #     plt.xlabel(f\"Feature {i}\")\n",
    "    # plt.suptitle(\"Feature Outlier Detection (Boxplots)\\n\\n\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # # Label Distribution\n",
    "    # plt.figure(figsize=(10, 4))\n",
    "    # sns.histplot(train_labels, kde=True, bins=30, color=\"blue\", label=\"Train Labels\")\n",
    "    # sns.histplot(test_labels, kde=True, bins=30, color=\"orange\", label=\"Test Labels\")\n",
    "    # plt.xlabel(\"Labels\")\n",
    "    # plt.ylabel(\"Count\")\n",
    "    # plt.legend()\n",
    "    # plt.title(\"Label Distribution (Train vs. Test)\\n\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3a561-9257-4255-a79c-1a313d637bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the Boston Housing dataset from Keras\n",
    "# Automatically splits into training and test sets (features and labels)\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use the analyze_dataset function to check the dataset\n",
    "analyze_dataset(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9418d3-b387-4761-8855-aa2ff62d59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Preprocessing The Dataset\n",
    "\n",
    "# 5.1. Reshape Labels\n",
    "\n",
    "# Convert labels to a 2D array with shape (-1, 1) to ensure compatibility with models\n",
    "train_labels = np.reshape(train_labels, newshape=(-1, 1))\n",
    "test_labels = np.reshape(test_labels, newshape=(-1, 1))\n",
    "print(\"\\nTrain Labels Shape:\", train_labels.shape)\n",
    "print(\"Test Labels Shape:\", test_labels.shape)\n",
    "\n",
    "# 5.2 Normalize Data using Min-Max Scaling\n",
    "\n",
    "# Check min/max values before normalization\n",
    "train_min, train_max = train_data.min(axis=0), train_data.max(axis=0)\n",
    "test_min, test_max = test_data.min(axis=0), test_data.max(axis=0)\n",
    "print(\"\\nTrain Data Min:\", train_min, \"\\nTrain Data Max:\", train_max)\n",
    "print(\"Test Data Min:\", test_min, \"\\nTest Data Max:\", test_max)\n",
    "\n",
    "# Fit the scaler on training data\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(train_data)\n",
    "\n",
    "# Transform both training and test data\n",
    "train_data = min_max_scaler.transform(train_data)\n",
    "test_data = min_max_scaler.transform(test_data)\n",
    "\n",
    "# Check min/max values after normalization\n",
    "train_min_post, train_max_post = train_data.min(axis=0), train_data.max(axis=0)\n",
    "test_min_post, test_max_post = test_data.min(axis=0), test_data.max(axis=0)\n",
    "print(\"\\nPost-Normalization Train Data Min:\", train_min_post, \"\\nPost-Normalization Train Data Max:\", train_max_post)\n",
    "print(\"Post-Normalization Test Data Min:\", test_min_post, \"\\nPost-Normalization Test Data Max:\", test_max_post)\n",
    "\n",
    "# 5.3. Change Data Types\n",
    "\n",
    "# Convert dataset values to float32 to optimize memory usage and computation speed\n",
    "train_data = train_data.astype(np.float32)\n",
    "test_data = test_data.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "print(\"\\nTrain Data Type:\", train_data.dtype)\n",
    "print(\"Test Data Type:\", test_data.dtype)\n",
    "print(\"Train Labels Type:\", train_labels.dtype)\n",
    "print(\"Test Labels Type:\", test_labels.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ddf4f-51d8-4cd4-be99-10a6a9aa6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create regression model\n",
    "\n",
    "# Define input layer\n",
    "input_layer = tf.keras.layers.Input(shape=(13,), batch_size=16)\n",
    "\n",
    "# Hidden layers with ReLU activation\n",
    "first_layer = tf.keras.layers.Dense(units=13, activation=\"relu\", use_bias=False)(input_layer)\n",
    "second_layer = tf.keras.layers.Dense(units=8, activation=\"relu\", use_bias=False)(first_layer)\n",
    "\n",
    "# Output layer (regression, so no activation)\n",
    "output_layer = tf.keras.layers.Dense(units=1)(second_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "regression_model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "regression_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
