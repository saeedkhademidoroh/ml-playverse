{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a685e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "\n",
    "# 1.1. Standard Libraries\n",
    "\n",
    "# 1.2. Third-party Libraries\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np  # Efficient array operations\n",
    "import pandas as pd  # Data structures for working with structured data\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt  # Plotting and data visualization\n",
    "import seaborn as sns  # Further data visualization\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scales data to a range [0, 1]\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf  # Core machine learning framework\n",
    "from keras.api.models import Model  # Model class\n",
    "from keras.api.layers import Input, Dense  # Layers for the model\n",
    "from keras.api.optimizers import Adam  # Optimizer for the model\n",
    "from keras.api.losses import MeanSquaredError  # Loss function for the model\n",
    "\n",
    "# 1.3. Local Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0141459-855c-4fb6-b2c3-ffc6e6639c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Function Definitions\n",
    "\n",
    "# 2.1 Dataset Functions\n",
    "\n",
    "# Function to analyze a dataset\n",
    "def analyze_dataset(train_data, test_data, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Perform a complete analysis of the dataset, including:\n",
    "    - Shape and data types\n",
    "    - Missing values\n",
    "    - Statistical summaries\n",
    "    - Feature distributions\n",
    "    - Correlation heatmap\n",
    "    - Outlier detection\n",
    "    - Label distribution\n",
    "\n",
    "    Parameters:\n",
    "        train_data (numpy.ndarray): Training feature set\n",
    "        test_data (numpy.ndarray): Testing feature set\n",
    "        train_labels (numpy.ndarray): Training labels\n",
    "        test_labels (numpy.ndarray): Testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to DataFrame for better analysis\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    train_labels_df = pd.DataFrame(train_labels, columns=[''])\n",
    "    test_labels_df = pd.DataFrame(test_labels, columns=[''])\n",
    "\n",
    "    # Dataset Shape and Data Types\n",
    "    print(\"\\nðŸ”¹ Dataset Shape & Data Types:\")\n",
    "    print(f\"Train data shape: {train_data.shape}, Type: {train_data.dtype}\")\n",
    "    print(f\"Test data shape: {test_data.shape}, Type: {test_data.dtype}\")\n",
    "    print(f\"Train labels shape: {train_labels.shape}, Type: {train_labels.dtype}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}, Type: {test_labels.dtype}\")\n",
    "\n",
    "    # Checking for Missing Values\n",
    "    print(\"\\nðŸ”¹ Missing Values:\")\n",
    "    print(f\"Train data missing values: {np.isnan(train_data).sum()}\")\n",
    "    print(f\"Test data missing values: {np.isnan(test_data).sum()}\")\n",
    "    print(f\"Train labels missing values: {np.isnan(train_labels).sum()}\")\n",
    "    print(f\"Test labels missing values: {np.isnan(test_labels).sum()}\")\n",
    "\n",
    "    # Summary Statistics (using DataFrame)\n",
    "    print(\"\\nðŸ”¹ Summary Statistics:\")\n",
    "    print(\"\\nTrain Data Statistics:\\n\\n\", train_df.describe())\n",
    "    print(\"\\nTest Data Statistics:\\n\\n\", test_df.describe())\n",
    "    print(\"\\nTrain Labels Statistics:\\n\", train_labels_df.describe())\n",
    "    print(\"\\nTest Labels Statistics:\\n\", test_labels_df.describe())\n",
    "\n",
    "    # Feature Distributions\n",
    "    num_features = train_data.shape[1]\n",
    "    plt.figure(figsize=(15, num_features * 2))\n",
    "    for i in range(num_features):\n",
    "        plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "        sns.histplot(train_data[:, i], kde=True, bins=30, color=\"blue\", label=\"Train\")\n",
    "        sns.histplot(test_data[:, i], kde=True, bins=30, color=\"orange\", label=\"Test\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "    plt.suptitle(\"Feature Distributions (Train vs. Test)\\n\\n\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr_matrix = train_df.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Feature Correlation Heatmap\\n\")\n",
    "    plt.show()\n",
    "\n",
    "    # Outlier Detection (Boxplots)\n",
    "    plt.figure(figsize=(15, num_features * 2))\n",
    "    for i in range(num_features):\n",
    "        plt.subplot((num_features // 3) + 1, 3, i + 1)\n",
    "        sns.boxplot(x=train_data[:, i], color=\"red\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "    plt.suptitle(\"Feature Outlier Detection (Boxplots)\\n\\n\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Label Distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(train_labels, kde=True, bins=30, color=\"blue\", label=\"Train Labels\")\n",
    "    sns.histplot(test_labels, kde=True, bins=30, color=\"orange\", label=\"Test Labels\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Label Distribution (Train vs. Test)\\n\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3a561-9257-4255-a79c-1a313d637bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the Boston Housing dataset from Keras\n",
    "# Automatically splits into training and test sets (features and labels)\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use the analyze_dataset function to check the dataset\n",
    "analyze_dataset(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9418d3-b387-4761-8855-aa2ff62d59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Preprocessing The Dataset\n",
    "\n",
    "# 5.1. Reshape Labels\n",
    "\n",
    "# Convert labels to a 2D array with shape (-1, 1) to ensure compatibility with models\n",
    "train_labels = np.reshape(train_labels, newshape=(-1, 1))\n",
    "test_labels = np.reshape(test_labels, newshape=(-1, 1))\n",
    "print(\"\\nTrain Labels Shape:\", train_labels.shape)\n",
    "print(\"Test Labels Shape:\", test_labels.shape)\n",
    "\n",
    "# 5.2 Normalize Data using Min-Max Scaling\n",
    "\n",
    "# Check min/max values for each set of data\n",
    "train_data_min, train_data_max = train_data.min(axis=0), train_data.max(axis=0)\n",
    "test_data_min, test_data_max = test_data.min(axis=0), test_data.max(axis=0)\n",
    "train_labels_min, train_labels_max = train_labels.min(axis=0), train_labels.max(axis=0)\n",
    "test_labels_min, test_labels_max = test_labels.min(axis=0), test_labels.max(axis=0)\n",
    "print(\"\\nTrain Data Min:\", train_data_min, \"\\nTrain Data Max:\", train_data_max)\n",
    "print(\"Test Data Min:\", test_data_min, \"\\nTest Data Max:\", test_data_max)\n",
    "print(\"Train Labels Min:\", train_labels_min, \"\\nTrain Labels Max:\", train_labels_max)\n",
    "print(\"Test Labels Min:\", test_labels_min, \"\\nTest Labels Max:\", test_labels_max)\n",
    "\n",
    "# Fit the scaler on training data\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(train_data)\n",
    "\n",
    "# Transform both training and test data\n",
    "train_data = min_max_scaler.transform(train_data)\n",
    "test_data = min_max_scaler.transform(test_data)\n",
    "\n",
    "# Check min/max values for training and test data after normalization\n",
    "train_min_post, train_max_post = train_data.min(axis=0), train_data.max(axis=0)\n",
    "test_min_post, test_max_post = test_data.min(axis=0), test_data.max(axis=0)\n",
    "print(\"\\nPost-Normalization Train Data Min:\", train_min_post, \"\\nPost-Normalization Train Data Max:\", train_max_post)\n",
    "print(\"Post-Normalization Test Data Min:\", test_min_post, \"\\nPost-Normalization Test Data Max:\", test_max_post)\n",
    "\n",
    "# 5.3. Change Data Types\n",
    "\n",
    "# Convert dataset values to float32tra to optimize memory usage and computation speed\n",
    "train_data = train_data.astype(np.float32)\n",
    "test_data = test_data.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "print(\"\\nTrain Data Type:\", train_data.dtype)\n",
    "print(\"Test Data Type:\", test_data.dtype)\n",
    "print(\"Train Labels Type:\", train_labels.dtype)\n",
    "print(\"Test Labels Type:\", test_labels.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ddf4f-51d8-4cd4-be99-10a6a9aa6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1. Create regression model with minimal setting\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Hidden layers with ReLU activation\n",
    "first_layer = Dense(units=4, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer (regression, so no activation)\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "regression_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36530b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2. Create regression model with 32 batch size for input\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(13,), batch_size=32)\n",
    "\n",
    "# Hidden layers with ReLU activation\n",
    "first_layer = Dense(units=4, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer (regression, so no activation)\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "regression_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3. Create regression model with 8 units in the hidden layer\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(13,))\n",
    "\n",
    "# Hidden layers with ReLU activation\n",
    "first_layer = Dense(units=8, activation=\"relu\")(input_layer)\n",
    "\n",
    "# Output layer (regression, so no activation)\n",
    "output_layer = Dense(units=1)(first_layer)\n",
    "\n",
    "# Define model using Functional API\n",
    "regression_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display model summary\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Compile the regression model with Adam optimizer and mean squared error loss\n",
    "regression_model.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Plot training labels and predictions before training\n",
    "plt.plot(train_labels[:30], \"r-\", label=\"y_true\")\n",
    "plt.plot(regression_model(train_data[:30]), \"b-\", label=\"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Plot test labels and predictions before training\n",
    "plt.plot(test_labels[:30], \"r-\", label=\"y_true\")\n",
    "plt.plot(regression_model(test_data[:30]), \"b-\", label=\"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Train the regression model on the training data\n",
    "regression_model.fit(x=train_data, y=train_labels, epochs=100, batch_size=8, validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Plot training labels and predictions after training\n",
    "plt.plot(train_labels[:30], \"r-\", label=\"y_true\")\n",
    "plt.plot(regression_model(train_data[:30]), \"b-\", label=\"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf63d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Plot test labels and predictions after training\n",
    "plt.plot(test_labels[:30], \"r-\", label=\"y_true\")\n",
    "plt.plot(regression_model(test_data[:30]), \"b-\", label=\"y_pred\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
